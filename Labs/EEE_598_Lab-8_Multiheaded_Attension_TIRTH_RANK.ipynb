{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9vk9Elugvmi"
      },
      "source": [
        "# **Notebook 12.2: Multihead Self-Attention**\n",
        "\n",
        "This notebook builds a multihead self-attention mechanism as in figure 12.6\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OLComQyvCIJ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OJkkoNqCVK2"
      },
      "source": [
        "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAygJwLiCSri",
        "outputId": "127c97cd-8f9a-4d92-cf12-1dcf61fa7145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.78862847  0.43650985  0.09649747 -1.8634927  -0.2773882  -0.35475898]\n",
            " [-0.08274148 -0.62700068 -0.04381817 -0.47721803 -1.31386475  0.88462238]\n",
            " [ 0.88131804  1.70957306  0.05003364 -0.40467741 -0.54535995 -1.54647732]\n",
            " [ 0.98236743 -1.10106763 -1.18504653 -0.2056499   1.48614836  0.23671627]\n",
            " [-1.02378514 -0.7129932   0.62524497 -0.16051336 -0.76883635 -0.23003072]\n",
            " [ 0.74505627  1.97611078 -1.24412333 -0.62641691 -0.80376609 -2.41908317]\n",
            " [-0.92379202 -1.02387576  1.12397796 -0.13191423 -1.62328545  0.64667545]\n",
            " [-0.35627076 -1.74314104 -0.59664964 -0.58859438 -0.8738823   0.02971382]]\n"
          ]
        }
      ],
      "source": [
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "# Number of inputs\n",
        "N = 6\n",
        "# Number of dimensions of each input\n",
        "D = 8\n",
        "# Create an empty list\n",
        "X = np.random.normal(size=(D,N))\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2iHFbtKMaDp"
      },
      "source": [
        "We'll use two heads.  We'll need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4).  We'll use two heads, and (as in the figure), we'll make the queries keys and values of size D/H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "79TSK7oLMobe"
      },
      "outputs": [],
      "source": [
        "# Number of heads\n",
        "H = 2\n",
        "# QDV dimension\n",
        "H_D = int(D/H)\n",
        "\n",
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Choose random values for the parameters for the first head\n",
        "omega_q1 = np.random.normal(size=(H_D,D))\n",
        "omega_k1 = np.random.normal(size=(H_D,D))\n",
        "omega_v1 = np.random.normal(size=(H_D,D))\n",
        "beta_q1 = np.random.normal(size=(H_D,1))\n",
        "beta_k1 = np.random.normal(size=(H_D,1))\n",
        "beta_v1 = np.random.normal(size=(H_D,1))\n",
        "\n",
        "# Choose random values for the parameters for the second head\n",
        "omega_q2 = np.random.normal(size=(H_D,D))\n",
        "omega_k2 = np.random.normal(size=(H_D,D))\n",
        "omega_v2 = np.random.normal(size=(H_D,D))\n",
        "beta_q2 = np.random.normal(size=(H_D,1))\n",
        "beta_k2 = np.random.normal(size=(H_D,1))\n",
        "beta_v2 = np.random.normal(size=(H_D,1))\n",
        "\n",
        "# Choose random values for the parameters\n",
        "omega_c = np.random.normal(size=(D,D))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxaKQtP3Ng6R"
      },
      "source": [
        "Now let's compute the multiscale self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "obaQBdUAMXXv"
      },
      "outputs": [],
      "source": [
        "# Define softmax operation that works independently on each column\n",
        "def softmax_cols(data_in):\n",
        "  # Exponentiate all of the values\n",
        "  exp_values = np.exp(data_in) ;\n",
        "  # Sum over columns\n",
        "  denom = np.sum(exp_values, axis = 0);\n",
        "  # Compute softmax (numpy broadcasts denominator to all rows automatically)\n",
        "  softmax = exp_values / denom\n",
        "  # return the answer\n",
        "  return softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gb2WvQ3SiH8r"
      },
      "outputs": [],
      "source": [
        " # Now let's compute self attention in matrix form\n",
        "def multihead_scaled_self_attention(X, omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1,\n",
        "                                     omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
        "    \"\"\"\n",
        "    Compute multihead scaled self-attention mechanism with 2 heads.\n",
        "\n",
        "    Args:\n",
        "        X: Input matrix of shape (sequence_length, embedding_dim)\n",
        "        omega_v1, omega_q1, omega_k1: Weight matrices for head 1 (projections)\n",
        "        beta_v1, beta_q1, beta_k1: Bias terms for head 1\n",
        "        omega_v2, omega_q2, omega_k2: Weight matrices for head 2 (projections)\n",
        "        beta_v2, beta_q2, beta_k2: Bias terms for head 2\n",
        "        omega_c: Concatenation weight matrix combining both heads\n",
        "\n",
        "    Returns:\n",
        "        X_prime: Output of multihead attention\n",
        "    \"\"\"\n",
        "\n",
        "    d_k = omega_k1.shape[1]\n",
        "\n",
        "    print(\"Computing Head 1...\")\n",
        "    V1 = X @ omega_v1 + beta_v1\n",
        "    Q1 = X @ omega_q1 + beta_q1\n",
        "    K1 = X @ omega_k1 + beta_k1\n",
        "\n",
        "    dot_products_1 = Q1 @ K1.T\n",
        "\n",
        "    scaled_dot_products_1 = dot_products_1 / np.sqrt(d_k)\n",
        "\n",
        "    attention_weights_1 = softmax(scaled_dot_products_1)\n",
        "\n",
        "    head1_output = attention_weights_1 @ V1\n",
        "\n",
        "    print(\"Computing Head 2...\")\n",
        "    V2 = X @ omega_v2 + beta_v2\n",
        "    Q2 = X @ omega_q2 + beta_q2\n",
        "    K2 = X @ omega_k2 + beta_k2\n",
        "\n",
        "    dot_products_2 = Q2 @ K2.T\n",
        "\n",
        "    scaled_dot_products_2 = dot_products_2 / np.sqrt(d_k)\n",
        "\n",
        "    attention_weights_2 = softmax(scaled_dot_products_2)\n",
        "\n",
        "    head2_output = attention_weights_2 @ V2\n",
        "\n",
        "    print(\"Concatenating and combining heads...\")\n",
        "    concatenated_output = np.concatenate([head1_output, head2_output], axis=1)\n",
        "\n",
        "    X_prime = concatenated_output @ omega_c\n",
        "\n",
        "    return X_prime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MUOJbgJskUpl"
      },
      "outputs": [],
      "source": [
        "# Run the self attention mechanism\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def multihead_scaled_self_attention(X, omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1,\n",
        "                                     omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
        "\n",
        "    print(\"Computing Head 1...\")\n",
        "    print(f\"X shape: {X.shape}\")\n",
        "    print(f\"omega_v1 shape: {omega_v1.shape}\")\n",
        "\n",
        "    V1 = X @ omega_v1 + beta_v1\n",
        "    Q1 = X @ omega_q1 + beta_q1\n",
        "    K1 = X @ omega_k1 + beta_k1\n",
        "\n",
        "    print(f\"V1 shape: {V1.shape}, Q1 shape: {Q1.shape}, K1 shape: {K1.shape}\")\n",
        "\n",
        "    dot_products_1 = Q1 @ K1.T\n",
        "    print(f\"dot_products_1 shape: {dot_products_1.shape}\")\n",
        "\n",
        "    d_k = K1.shape[1]\n",
        "    scaled_dot_products_1 = dot_products_1 / np.sqrt(d_k)\n",
        "\n",
        "    attention_weights_1 = softmax(scaled_dot_products_1)\n",
        "    print(f\"attention_weights_1 shape: {attention_weights_1.shape}\")\n",
        "\n",
        "    head1_output = attention_weights_1 @ V1\n",
        "    print(f\"head1_output shape: {head1_output.shape}\")\n",
        "\n",
        "    print(\"\\nComputing Head 2...\")\n",
        "    V2 = X @ omega_v2 + beta_v2\n",
        "    Q2 = X @ omega_q2 + beta_q2\n",
        "    K2 = X @ omega_k2 + beta_k2\n",
        "\n",
        "    print(f\"V2 shape: {V2.shape}, Q2 shape: {Q2.shape}, K2 shape: {K2.shape}\")\n",
        "\n",
        "    dot_products_2 = Q2 @ K2.T\n",
        "    print(f\"dot_products_2 shape: {dot_products_2.shape}\")\n",
        "\n",
        "    d_k = K2.shape[1]\n",
        "    scaled_dot_products_2 = dot_products_2 / np.sqrt(d_k)\n",
        "\n",
        "    attention_weights_2 = softmax(scaled_dot_products_2)\n",
        "    print(f\"attention_weights_2 shape: {attention_weights_2.shape}\")\n",
        "\n",
        "    head2_output = attention_weights_2 @ V2\n",
        "    print(f\"head2_output shape: {head2_output.shape}\")\n",
        "\n",
        "    print(\"\\nConcatenating and combining heads...\")\n",
        "    concatenated_output = np.concatenate([head1_output, head2_output], axis=1)\n",
        "    print(f\"concatenated_output shape: {concatenated_output.shape}\")\n",
        "    print(f\"omega_c shape: {omega_c.shape}\")\n",
        "\n",
        "    X_prime = concatenated_output @ omega_c\n",
        "    print(f\"X_prime shape: {X_prime.shape}\")\n",
        "\n",
        "    return X_prime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set seed so we get the same random numbers\n",
        "np.random.seed(3)\n",
        "\n",
        "# Number of inputs and dimensions\n",
        "N = 6\n",
        "D = 8\n",
        "\n",
        "# Inputs matrix (each column is a token/vector)\n",
        "X = np.random.normal(size=(D, N))\n",
        "print(X)\n",
        "\n",
        "# Number of heads\n",
        "H = 2\n",
        "# Per-head Q/K/V dimension\n",
        "H_D = int(D / H)\n",
        "\n",
        "# Set seed so we get the same random numbers for parameters\n",
        "np.random.seed(0)\n",
        "\n",
        "# Head 1 parameters\n",
        "omega_q1 = np.random.normal(size=(H_D, D))\n",
        "omega_k1 = np.random.normal(size=(H_D, D))\n",
        "omega_v1 = np.random.normal(size=(H_D, D))\n",
        "beta_q1  = np.random.normal(size=(H_D, 1))\n",
        "beta_k1  = np.random.normal(size=(H_D, 1))\n",
        "beta_v1  = np.random.normal(size=(H_D, 1))\n",
        "\n",
        "# Head 2 parameters\n",
        "omega_q2 = np.random.normal(size=(H_D, D))\n",
        "omega_k2 = np.random.normal(size=(H_D, D))\n",
        "omega_v2 = np.random.normal(size=(H_D, D))\n",
        "beta_q2  = np.random.normal(size=(H_D, 1))\n",
        "beta_k2  = np.random.normal(size=(H_D, 1))\n",
        "beta_v2  = np.random.normal(size=(H_D, 1))\n",
        "\n",
        "# Output projection\n",
        "omega_c = np.random.normal(size=(D, D))\n",
        "\n",
        "# Column-wise softmax (stable)\n",
        "def softmax_cols(data_in):\n",
        "    shifted = data_in - np.max(data_in, axis=0, keepdims=True)\n",
        "    exp_values = np.exp(shifted)\n",
        "    denom = np.sum(exp_values, axis=0, keepdims=True)\n",
        "    return exp_values / denom\n",
        "\n",
        "# Multihead scaled dot-product self-attention\n",
        "def multihead_scaled_self_attention(\n",
        "    X,\n",
        "    omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1,\n",
        "    omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2,\n",
        "    omega_c\n",
        "):\n",
        "    D, N = X.shape\n",
        "    H_D = omega_q1.shape[0]\n",
        "    ones_row = np.ones((1, N))\n",
        "\n",
        "    # ---- Head 1 ----\n",
        "    Q1 = omega_q1 @ X + beta_q1 @ ones_row        # (H_D, N)\n",
        "    K1 = omega_k1 @ X + beta_k1 @ ones_row        # (H_D, N)\n",
        "    V1 = omega_v1 @ X + beta_v1 @ ones_row        # (H_D, N)\n",
        "\n",
        "    scores1 = K1.T @ Q1                            # (N, N) with [m, n] = k_m^T q_n\n",
        "    scores1 = scores1 * (1.0 / np.sqrt(H_D))       # scale by sqrt(d_k)\n",
        "    A1 = softmax_cols(scores1)                     # (N, N)\n",
        "    O1 = V1 @ A1                                   # (H_D, N)\n",
        "\n",
        "    # ---- Head 2 ----\n",
        "    Q2 = omega_q2 @ X + beta_q2 @ ones_row        # (H_D, N)\n",
        "    K2 = omega_k2 @ X + beta_k2 @ ones_row        # (H_D, N)\n",
        "    V2 = omega_v2 @ X + beta_v2 @ ones_row        # (H_D, N)\n",
        "\n",
        "    scores2 = K2.T @ Q2                            # (N, N)\n",
        "    scores2 = scores2 * (1.0 / np.sqrt(H_D))       # scale by sqrt(d_k)\n",
        "    A2 = softmax_cols(scores2)                     # (N, N)\n",
        "    O2 = V2 @ A2                                   # (H_D, N)\n",
        "\n",
        "    # Concatenate head outputs and project\n",
        "    O = np.vstack([O1, O2])                        # (D, N)\n",
        "    X_prime = omega_c @ O                          # (D, N)\n",
        "\n",
        "    return X_prime\n",
        "\n",
        "# Run the self attention mechanism\n",
        "X_prime = multihead_scaled_self_attention(\n",
        "    X,\n",
        "    omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1,\n",
        "    omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2,\n",
        "    omega_c\n",
        ")\n",
        "\n",
        "# Print out the results\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "print(\"My answer:\")\n",
        "print(X_prime)\n",
        "\n",
        "print(\"\\nTrue values:\")\n",
        "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
        "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
        "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
        "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
        "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
        "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
        "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\")\n",
        "print(\" [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F7JcLvcRp4Z",
        "outputId": "d8103c66-8a25-4f76-8dc3-2287c106ca46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.789  0.437  0.096 -1.863 -0.277 -0.355]\n",
            " [-0.083 -0.627 -0.044 -0.477 -1.314  0.885]\n",
            " [ 0.881  1.71   0.05  -0.405 -0.545 -1.546]\n",
            " [ 0.982 -1.101 -1.185 -0.206  1.486  0.237]\n",
            " [-1.024 -0.713  0.625 -0.161 -0.769 -0.23 ]\n",
            " [ 0.745  1.976 -1.244 -0.626 -0.804 -2.419]\n",
            " [-0.924 -1.024  1.124 -0.132 -1.623  0.647]\n",
            " [-0.356 -1.743 -0.597 -0.589 -0.874  0.03 ]]\n",
            "My answer:\n",
            "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
            " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
            " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
            " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
            " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
            " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
            " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\n",
            " [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\n",
            "\n",
            "True values:\n",
            "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
            " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
            " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
            " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
            " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
            " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
            " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\n",
            " [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rz8SZVPRqbE"
      },
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}